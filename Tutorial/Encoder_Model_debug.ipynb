{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\qwsaz\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_BINS = 60 * 3\n",
    "ZMIN = 0.0\n",
    "ZMAX = 0.4\n",
    "BIN_SIZE = (ZMAX - ZMIN) / NB_BINS\n",
    "range_z = np.linspace(ZMIN, ZMAX, NB_BINS + 1)[:NB_BINS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['specObjID','z','dered_u','dered_g','dered_r','dered_i','dered_z']\n",
    "\n",
    "X = pd.read_csv('..//..//photo_z//DATA//SDSS_DR12_awe2_0.csv',usecols=cols)\n",
    "\n",
    "X.drop_duplicates(subset=['specObjID'],inplace=True)\n",
    "\n",
    "#Labels1 is a dictionary of spectraID to Z\n",
    "\n",
    "#labels2 is a dictionary of spectraID to an array np.array([u,g,r,i,z])\n",
    "\n",
    "redshift_dictionary = dict(zip((X['specObjID'].values).tolist(),(X['z'].values).tolist()))\n",
    "\n",
    "magnitude_dictionary = dict(zip((X['specObjID'].values).tolist(), \\\n",
    "                                (X[['dered_u','dered_g','dered_r','dered_i','dered_z']].values).tolist()))\n",
    "\n",
    "X = X[X['z']<=0.4]\n",
    "X.reset_index(inplace=True)\n",
    "Y = X['z'].values\n",
    "NOW = X.drop(columns=['specObjID','z','index'])\n",
    "\n",
    "def make_cat(Y):\n",
    "    return tf.keras.utils.to_categorical(np.round((180-1)*(Y/0.4),0).astype(int))\n",
    "\n",
    "#grab a reserved testing set, of 300000 values\n",
    "x_train=NOW.iloc[0:175000].values\n",
    "y_train=make_cat(Y[0:175000])\n",
    "\n",
    "x_val=NOW.iloc[175000:300000].values\n",
    "y_val=make_cat(Y[175000:300000])\n",
    "\n",
    "x_test = NOW.iloc[300000::].values\n",
    "y_test= make_cat(Y[300000::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels1, labels2, batch_size=64, dim=(3800), n_channels=1,\n",
    "                 n_classes=2, shuffle=True):\n",
    "     #   'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels1 = labels1\n",
    "        self.labels2 = labels2\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "    #'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        #'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, *self.dim), dtype=np.float32)\n",
    "        y1 = np.empty((self.batch_size,5))\n",
    "        y2 = np.empty((self.batch_size))\n",
    "        # Generate data and perform augmentation\n",
    "        for i,ID in enumerate(list_IDs_temp):\n",
    "            #print(list_IDs_temp)\n",
    "            # Store sample\n",
    "            X[i,] = np.load('..//Spectra//' + str(ID) + '.npy')[0:3800]\n",
    "            \n",
    "            #store magnitude target\n",
    "            y1[i,] = self.labels1[ID]\n",
    "            #store spectro target\n",
    "            y2[i] = self.labels2[ID]\n",
    "        \n",
    "        if self.n_classes > 2:\n",
    "            y2 = (np.round((y2/0.4) * (self.n_classes-1),0)).astype(int) #noramlize, bin\n",
    "            #y[self.n_channels<=y] = self.n_classes-1 I think this is fucking with me, should already be taken care of below\n",
    "            #y = keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "            return (X, [y1,keras.utils.to_categorical(y2, num_classes=self.n_classes)])\n",
    "        else:\n",
    "            return (X, [y1,y2])\n",
    "    \n",
    "    def __len__(self):\n",
    "    #'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    #  'Generate one batch of data'\n",
    "      # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "      # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "      # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8\n",
    "\n",
    "          \n",
    "params_train = {'dim': (3800,),\n",
    "          'batch_size': 8,\n",
    "          'n_classes': 180,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = os.listdir('..//Spectra')\n",
    "ID_list = [sub[ : -4] for sub in ID_list]\n",
    "ID_list = np.array(ID_list,dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culling IDs for targets > 0.4, length before:  1000\n",
      "length after:  999\n"
     ]
    }
   ],
   "source": [
    "#great now remove all ID whose target is above 0.4:\n",
    "keep_indices=[]\n",
    "print('culling IDs for targets > 0.4, length before: ',len(ID_list))\n",
    "for i,val in enumerate(ID_list):\n",
    "    if redshift_dictionary[val]<=0.4: #to be most fair, I'd like it to just set all z above 0.4 to 0.4 in real life...\n",
    "        keep_indices.append(i)        \n",
    "ID_list=ID_list[keep_indices]\n",
    "print('length after: ',len(ID_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = ID_list[0:100]\n",
    "val_list = ID_list[100:200]\n",
    "train_list = ID_list[200::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {'train':train_list,'validation':val_list,'test':test_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(list_IDs=partition['train'], labels1=magnitude_dictionary, labels2=redshift_dictionary, **params_train)\n",
    "#val_generator = DataGenerator(list_IDs=partition['validation'], labels1=magnitude_dictionary, labels2=redshift_dictionary, **params_test)\n",
    "#test_generator = DataGenerator(list_IDs=partition['test'], labels1=magnitude_dictionary, labels2=redshift_dictionary, **params_test)\n",
    "\n",
    "train_steps_to_take = int(len(train_list)/BATCH_SIZE)\n",
    "#val_steps_to_take = int(len(val_list)/BATCH_SIZE)\n",
    "#test_steps_to_take = int(len(test_list)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.layers import Layer\\n#\\nclass Pairwise_Subtraction(Layer):\\n\\n    def __init__(self, output_dim=15, **kwargs):\\n        self.output_dim = output_dim\\n        super(Pairwise_Subtraction, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):\\n        # Create a trainable weight variable for this layer.\\n        self.kernel = self.add_weight(name='kernel', \\n                                      shape=(input_shape[1], self.output_dim),\\n                                      initializer=keras.initializers.Ones(),\\n                                      trainable=False)\\n        super(Pairwise_Subtraction, self).build(input_shape)  # Be sure to call this at the end\\n\\n    def call(self, x):\\n        out = np.ones((input_shape,15))\\n        out = tf.convert_to_tensor(out, dtype=tf.float32)\\n        out[0:5]=x\\n        out[5] = x[0]-x[1]\\n        out[6] = x[1]-x[2]\\n        out[7] = x[2]-x[3]\\n        out[8] = x[3]-x[4]\\n        out[9] = x[0]-x[2]\\n        out[10] = x[0]-x[3]\\n        out[11] = x[0]-x[4]\\n        out[12] = x[1]-x[3]\\n        out[13] = x[1]-x[4]\\n        out[14] = x[2]-x[4]\\n        return(keras.matmul(out, self.kernel))\\n    def compute_output_shape(self, input_shape):\\n        return (input_shape[0], self.output_dim)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from keras.layers import Layer\n",
    "#\n",
    "class Pairwise_Subtraction(Layer):\n",
    "\n",
    "    def __init__(self, output_dim=15, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Pairwise_Subtraction, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer=keras.initializers.Ones(),\n",
    "                                      trainable=False)\n",
    "        super(Pairwise_Subtraction, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        out = np.ones((input_shape,15))\n",
    "        out = tf.convert_to_tensor(out, dtype=tf.float32)\n",
    "        out[0:5]=x\n",
    "        out[5] = x[0]-x[1]\n",
    "        out[6] = x[1]-x[2]\n",
    "        out[7] = x[2]-x[3]\n",
    "        out[8] = x[3]-x[4]\n",
    "        out[9] = x[0]-x[2]\n",
    "        out[10] = x[0]-x[3]\n",
    "        out[11] = x[0]-x[4]\n",
    "        out[12] = x[1]-x[3]\n",
    "        out[13] = x[1]-x[4]\n",
    "        out[14] = x[2]-x[4]\n",
    "        return(keras.matmul(out, self.kernel))\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model():\n",
    "    Input = keras.layers.Input((3800,1),)\n",
    "    conv1 = keras.layers.Conv1D(filters=32,kernel_size=7,dilation_rate=1)(Input)\n",
    "    pool1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = keras.layers.Conv1D(filters=64,kernel_size=3,dilation_rate=1)(conv1)\n",
    "    pool2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    edense1 = keras.layers.Dense(252,activation=keras.activations.relu)(pool2)\n",
    "    edense2 = keras.layers.Dense(128,activation=keras.activations.relu)(edense1)\n",
    "    edense3 = keras.layers.Dense(5,activation=keras.activations.linear,name='output1')(edense2)\n",
    "    \n",
    "    dense1 = keras.layers.Dense(45,activation=keras.activations.relu)(edense3)\n",
    "    drop1 = keras.layers.Dropout(0.05)(dense1)\n",
    "    \n",
    "    dense2 = keras.layers.Dense(45,activation=keras.activations.relu)(drop1)\n",
    "    drop2 = keras.layers.Dropout(0.05)(dense2) #0.1\n",
    "    \n",
    "    dense3 = keras.layers.Dense(45,activation=keras.activations.relu)(drop2)\n",
    "    drop3 = keras.layers.Dropout(0.05)(dense3)\n",
    "    \n",
    "    dense4 = keras.layers.Dense(45,activation=keras.activations.relu)(drop3)\n",
    "    drop4 = keras.layers.Dropout(0.05)(dense4)#0.1\n",
    "    \n",
    "    dense5 = keras.layers.Dense(180,activation=keras.activations.softmax,name='output2')(drop4)\n",
    "    \n",
    "    model = keras.Model(inputs=[Input],outputs=[edense3,dense5])\n",
    "    return model\n",
    "\n",
    "def MLP():\n",
    "    IN = keras.layers.Input((5,))\n",
    "    dense1 = keras.layers.Dense(45,activation=keras.activations.relu)(IN)\n",
    "    drop1 = keras.layers.Dropout(0.05)(dense1)\n",
    "    \n",
    "    dense2 = keras.layers.Dense(45,activation=keras.activations.relu)(drop1)\n",
    "    drop2 = keras.layers.Dropout(0.05)(dense2) #0.1\n",
    "    \n",
    "    dense3 = keras.layers.Dense(45,activation=keras.activations.relu)(drop2)\n",
    "    drop3 = keras.layers.Dropout(0.05)(dense3)\n",
    "    \n",
    "    dense4 = keras.layers.Dense(45,activation=keras.activations.relu)(drop3)\n",
    "    drop4 = keras.layers.Dropout(0.05)(dense4)#0.1\n",
    "    \n",
    "    dense5 = keras.layers.Dense(180,activation=keras.activations.softmax)(drop4)\n",
    "    \n",
    "    model = keras.Model(inputs=[IN],outputs=[dense5])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='SPECTRAL_ENCODER_MODEL.hdf5'\n",
    "encoder = encoder_model()\n",
    "adam_e = tf.keras.optimizers.Adam(lr=1e-4)#5e-4 unstable, reduce\n",
    "\n",
    "losses = {\n",
    "    \"output1\": \"MSE\",\n",
    "    \"output2\": \"categorical_crossentropy\",\n",
    "}\n",
    "\n",
    "lossWeights = {\"output1\": 1.0, \"output2\": 1.0}\n",
    "\n",
    "encoder.compile(optimizer=adam_e, loss=losses, loss_weights=lossWeights)\n",
    "\n",
    "predictor = MLP()\n",
    "adam_p = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "\n",
    "predictor.compile(optimizer=adam_p,loss='categorical_crossentropy')\n",
    "\n",
    "#compare s=0.1 to MSE; fucked up it was (2*0.1)**2 istead of 2*(0.1)**2.account for that if it matters to us in an hour\n",
    "\n",
    "#Need to find where my guy overtrains and strike a balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125000/125000 [==============================] - 8s 64us/sample - loss: 5.3743\n",
      "best loss beaten, saving\n",
      "125000/125000 [==============================] - 8s 63us/sample - loss: 5.3743\n",
      "best loss beaten, saving\n",
      "125000/125000 [==============================] - 8s 63us/sample - loss: 5.3743\n",
      "best loss beaten, saving\n",
      " 20448/125000 [===>..........................] - ETA: 7s - loss: 5.3767"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-bdc2e9aa0186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mnew_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best loss beaten, saving'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1048\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m           callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m   def predict(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3217\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 415\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    416\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DAMLA\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath='predictor_weights.hdf5'\n",
    "\n",
    "best_loss = 1000\n",
    "for i in range(1,30):\n",
    "    encoder.fit_generator(generator=train_generator,\n",
    "                       steps_per_epoch=train_steps_to_take,\n",
    "                       epochs=1,\n",
    "                       initial_epoch=i,\n",
    "                       verbose=1)\n",
    "    \n",
    "    weights1 = encoder.get_weights()\n",
    "    predictor.set_weights(weights1[10::])\n",
    "    \n",
    "    new_loss = predictor.evaluate(x=x_val,y=y_val,verbose=1)\n",
    "    if new_loss < best_loss:\n",
    "        print('best loss beaten, saving')\n",
    "        predictor.save_weights(filepath)\n",
    "        best_loss=new_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
